Сорвиголова, [17.12.2025 18:54]
"""
Реализация метода опорных векторов (SVM) с нуля для бинарной классификации
"""

import numpy as np

class SimpleSVM:
    """
    Простой SVM для бинарной классификации с линейным ядром
    """
    def __init__(self, learning_rate=0.001, n_iters=1000):
        """
        Инициализация параметров SVM
        
        Параметры:
        learning_rate: скорость обучения (шаг градиентного спуска)
        n_iters: количество итераций обучения
        """
        self.lr = learning_rate  # Скорость обучения
        self.n_iters = n_iters   # Количество итераций
        self.w = None            # Вектор весов (нормаль к гиперплоскости)
        self.b = None            # Смещение (bias)
    
    def fit(self, X, y):
        """
        Обучение SVM на предоставленных данных
        
        Параметры:
        X: матрица признаков размером (n_samples, n_features)
        y: вектор меток классов (должны быть -1 и 1, или 0 и 1)
        """
        # Получаем количество образцов и признаков
        n_samples, n_features = X.shape
        
        # Преобразуем метки классов в формат -1 и 1
        # Если в y используются 0 и 1, преобразуем 0 в -1
        y_ = np.where(y <= 0, -1, 1)
        
        # Инициализация весов нулями
        # w - вектор нормали к гиперплоскости
        self.w = np.zeros(n_features)
        
        # Инициализация смещения
        self.b = 0
        
        # Градиентный спуск для оптимизации функции потерь
        for iteration in range(self.n_iters):
            # Проходим по всем образцам (стохастический градиентный спуск)
            for idx, x_i in enumerate(X):
                # x_i - текущий образец
                # Вычисляем условие для образца
                # y_i * (w·x_i - b) >= 1 означает, что образец правильно классифицирован
                # и находится за пределами границы зазора
                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1
                
                # Если условие выполняется, образец правильно классифицирован
                # с достаточным зазором
                if condition:
                    # Обновляем только веса с регуляризацией (L2 регуляризация)
                    # Цель: минимизировать норму весов ||w||^2
                    self.w -= self.lr * (2 * 0.01 * self.w)  # 0.01 - параметр регуляризации
                else:
                    # Если условие не выполняется, образец неправильно классифицирован
                    # или находится внутри границы зазора
                    # Обновляем и веса, и смещение
                    self.w -= self.lr * (2 * 0.01 * self.w - np.dot(x_i, y_[idx]))
                    self.b -= self.lr * y_[idx]
    
    def predict(self, X):
        """
        Предсказание классов для новых данных
        
        Параметры:
        X: матрица признаков для предсказания
        
        Возвращает:
        Вектор предсказанных меток классов (-1 или 1)
        """
        # Вычисляем решающую функцию: f(x) = w·x - b
        # Знак f(x) определяет класс:
        # f(x) >= 0 → класс 1
        # f(x) < 0 → класс -1
        linear_output = np.dot(X, self.w) - self.b
        
        # Применяем функцию знака для получения предсказаний
        return np.sign(linear_output)


# ============================================
# ПРИМЕР ИСПОЛЬЗОВАНИЯ SVM
# ============================================

# Создаем простой набор данных для классификации
# Два класса, линейно разделимые
X = np.array([
    # Класс -1 (синие точки)
    [1, 2],  # Образец 1 класса -1
    [2, 3],  # Образец 2 класса -1
    [3, 1],  # Образец 3 класса -1
    
    # Класс 1 (красные точки)
    [6, 5],  # Образец 1 класса 1
    [7, 7],  # Образец 2 класса 1
    [8, 6]   # Образец 3 класса 1
])

# Метки классов: первые три образца - класс -1, последние три - класс 1
y = np.array([-1, -1, -1, 1, 1, 1])

# Создаем экземпляр SVM
# learning_rate=0.001 - маленькая скорость обучения для стабильности
# n_iters=1000 - 1000 итераций обучения
svm = SimpleSVM(learning_rate=0.001, n_iters=1000)

Сорвиголова, [17.12.2025 18:54]
# Обучаем SVM на данных
print("Обучение SVM...")
svm.fit(X, y)

# Делаем предсказания на тех же данных (для демонстрации)
print("\nПредсказания на обучающих данных:")
predictions = svm.predict(X)
print("Предсказанные метки:", predictions)
print("Истинные метки:    ", y)
print("Совпадение предсказаний:", np.mean(predictions == y) * 100, "%")

# Выводим параметры обученной модели
print("\nПараметры обученной модели:")
print("Вектор весов w (нормаль к гиперплоскости):", svm.w)
print("Смещение b:", svm.b)

# Пример предсказания для нового образца
print("\nПример предсказания для нового образца:")
new_sample = np.array([[4, 4]])  # Точка между классами
prediction = svm.predict(new_sample)
print(f"Образец: {new_sample[0]}, Предсказанный класс: {prediction[0]}")

# Проверка уравнения гиперплоскости
print("\nПроверка решающей функции для образцов:")
for i in range(len(X)):
    score = np.dot(X[i], svm.w) - svm.b
    print(f"Образец {i+1}: X={X[i]}, f(x)={score:.2f}, Класс={y[i]}, Предсказание={predictions[i]}")
